# multimodal_rag_app.py (í”„ë¡œì íŠ¸ ì§€ì • ì˜¤ë¥˜ í•´ê²°ëœ ìµœì¢… ë²„ì „)
import streamlit as st
from google.oauth2 import service_account
from google.cloud import firestore
from datetime import datetime
import base64
import mimetypes
import vertexai # <--- import ì¶”ê°€

from langchain_google_vertexai import VertexAIEmbeddings, ChatVertexAI
from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage
from PIL import Image

# --- ì¸ì¦ ë° ì´ˆê¸°í™” ---
try:
    # Streamlit Secretsì—ì„œ ê° í•­ëª©ì„ ê°œë³„ì ìœ¼ë¡œ ì½ì–´ì™€ ë”•ì…”ë„ˆë¦¬ë¡œ ì¡°ë¦½
    creds_dict = {
        "type": st.secrets.firestore.type,
        "project_id": st.secrets.firestore.project_id,
        "private_key_id": st.secrets.firestore.private_key_id,
        "private_key": st.secrets.firestore.private_key.replace('\\n', '\n'),
        "client_email": st.secrets.firestore.client_email,
        "client_id": st.secrets.firestore.client_id,
        "auth_uri": st.secrets.firestore.auth_uri,
        "token_uri": st.secrets.firestore.token_uri,
        "auth_provider_x509_cert_url": st.secrets.firestore.auth_provider_x509_cert_url,
        "client_x509_cert_url": st.secrets.firestore.client_x509_cert_url
    }
    creds = service_account.Credentials.from_service_account_info(creds_dict)
    
    # Firestore í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    db = firestore.Client(credentials=creds, project=st.secrets.firestore.project_id)

    # Vertex AI ì „ì—­ ì´ˆê¸°í™” (ê°€ì¥ ì¤‘ìš”!)
    vertexai.init(project=st.secrets.firestore.project_id, credentials=creds)

except Exception as e:
    st.error("âš ï¸ ì¸ì¦ ì •ë³´ ì„¤ì •ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. Streamlit Cloudì˜ Secrets ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.error(f"ì˜¤ë¥˜ ìƒì„¸: {e}")
    st.stop()

# --- í—¬í¼ í•¨ìˆ˜ ë° í•µì‹¬ ë¡œì§ (ì´í•˜ ë¶€ë¶„ì€ ì´ì „ê³¼ ë™ì¼) ---
def image_to_base64_uri(file_path_or_bytes):
    if isinstance(file_path_or_bytes, str):
        mime_type, _ = mimetypes.guess_type(file_path_or_bytes)
        if mime_type is None: mime_type = 'image/jpeg'
        with open(file_path_or_bytes, "rb") as image_file:
            bytes_data = image_file.read()
    else:
        mime_type = file_path_or_bytes.type
        bytes_data = file_path_or_bytes.getvalue()
    encoded_string = base64.b64encode(bytes_data).decode("utf-8")
    return f"data:{mime_type};base64,{encoded_string}"

@st.cache_resource
def load_retriever():
    embedding_model = VertexAIEmbeddings(model_name="multimodalembedding@001")
    vector_store = FAISS.load_local("faiss_multimodal_index", embedding_model, allow_dangerous_deserialization=True)
    return vector_store.as_retriever(search_kwargs={"k": 3})

def get_answer_from_llm(retriever, query_text=None, query_image=None):
    retrieval_input = query_text if query_text else Image.open(query_image)
    relevant_docs = retriever.invoke(retrieval_input)

    context_content = []
    st.write("---")
    st.markdown("#### ğŸ“š ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ (Context):")
    for doc in relevant_docs:
        if doc.metadata['type'] == 'text':
            context_content.append({"type": "text", "text": f"ì°¸ê³  í…ìŠ¤íŠ¸: {doc.page_content}"})
            st.text(f"[í…ìŠ¤íŠ¸] {doc.metadata['source']}")
        elif doc.metadata['type'] == 'image':
            image_uri = image_to_base64_uri(doc.metadata['path'])
            context_content.append({"type": "image_url", "image_url": {"url": image_uri}})
            st.image(doc.metadata['path'], caption=f"[ì´ë¯¸ì§€] {doc.metadata['source']}", width=200)

    prompt_text = "ë‹¹ì‹ ì€ ì œë¯¸ë‹ˆ AI ì—°êµ¬ì†Œì˜ ê·œì • ì•ˆë‚´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ 'ê´€ë ¨ ì •ë³´'ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ 'ì§ˆë¬¸'ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
    
    user_original_query = [{"type": "text", "text": "--- ì§ˆë¬¸ ---"}]
    if query_text:
        user_original_query.append({"type": "text", "text": query_text})
    if query_image:
        image_uri = image_to_base64_uri(query_image)
        user_original_query.append({"type": "image_url", "image_url": {"url": image_uri}})

    final_prompt_content = [
        {"type": "text", "text": prompt_text},
        {"type": "text", "text": "--- ê´€ë ¨ ì •ë³´ ---"},
        *context_content,
        *user_original_query
    ]
    
    llm = ChatVertexAI(model_name="gemini-1.0-pro-vision", location="asia-northeast3")
    message = HumanMessage(content=final_prompt_content)
    response = llm.invoke([message])
    return response.content

# --- Streamlit UI ---
st.set_page_config(page_title="ë©€í‹°ëª¨ë‹¬ RAG ì±—ë´‡", page_icon="ğŸ§ ")
st.title("ğŸ§  ë©€í‹°ëª¨ë‹¬ RAG ì±—ë´‡")
st.markdown("í…ìŠ¤íŠ¸ë‚˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°êµ¬ì†Œ ê·œì •ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")

try:
    retriever = load_retriever()
    query_text = st.text_input("í…ìŠ¤íŠ¸ë¡œ ì§ˆë¬¸í•˜ê¸°:")
    query_image = st.file_uploader("ì´ë¯¸ì§€ë¡œ ì§ˆë¬¸í•˜ê¸°:", type=["jpg", "jpeg", "png"])

    if st.button("ë‹µë³€ ìƒì„±"):
        if not query_text and not query_image:
            st.warning("í…ìŠ¤íŠ¸ë‚˜ ì´ë¯¸ì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ê³  Geminiê°€ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                if query_image:
                    st.image(query_image, caption="ì§ˆë¬¸ ì´ë¯¸ì§€")
                answer = get_answer_from_llm(retriever, query_text, query_image)
                st.markdown("---")
                st.markdown("#### âœ¨ ìµœì¢… ë‹µë³€:")
                st.markdown(answer)
except FileNotFoundError:
    st.error("`faiss_multimodal_index` ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì´ë¼ë©´, ë¨¼ì € `indexer.py`ë¥¼ ì‹¤í–‰í•˜ì—¬ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. í´ë¼ìš°ë“œ ë°°í¬ ì‹œì—ëŠ” ì¸ë±ìŠ¤ íŒŒì¼ ì—…ë¡œë“œ ë°©ë²•ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.")
except Exception as e:
    st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")